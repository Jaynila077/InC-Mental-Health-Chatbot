{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n\n#nltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer= WordNetLemmatizer()\n\n# Modelling\nfrom sklearn.model_selection import train_test_split,KFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\n\n#Lime\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\nfrom lime.lime_text import IndexedString,IndexedCharacters\nfrom lime.lime_base import LimeBase\nfrom lime.lime_text import explanation\nsns.set(font_scale=1.3)\nnltk.download('omw-1.4')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-01-05T11:16:04.027289Z","iopub.execute_input":"2023-01-05T11:16:04.027704Z","iopub.status.idle":"2023-01-05T11:16:04.251182Z","shell.execute_reply.started":"2023-01-05T11:16:04.027667Z","shell.execute_reply":"2023-01-05T11:16:04.250283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read datasets\ndf_train = pd.read_csv('/kaggle/input/emotions-dataset-for-nlp/train.txt', names=['Text', 'Emotion'], sep=';')\ndf_val = pd.read_csv('/kaggle/input/emotions-dataset-for-nlp/val.txt', names=['Text', 'Emotion'], sep=';')\ndf_test = pd.read_csv('/kaggle/input/emotions-dataset-for-nlp/test.txt', names=['Text', 'Emotion'], sep=';')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.483044Z","iopub.execute_input":"2023-01-05T10:09:53.483999Z","iopub.status.idle":"2023-01-05T10:09:53.532993Z","shell.execute_reply.started":"2023-01-05T10:09:53.483969Z","shell.execute_reply":"2023-01-05T10:09:53.531888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print first 5 rows\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.534437Z","iopub.execute_input":"2023-01-05T10:09:53.535264Z","iopub.status.idle":"2023-01-05T10:09:53.547649Z","shell.execute_reply.started":"2023-01-05T10:09:53.535228Z","shell.execute_reply":"2023-01-05T10:09:53.546814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the shape of the data set\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.550607Z","iopub.execute_input":"2023-01-05T10:09:53.551257Z","iopub.status.idle":"2023-01-05T10:09:53.558026Z","shell.execute_reply.started":"2023-01-05T10:09:53.55122Z","shell.execute_reply":"2023-01-05T10:09:53.556934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print first 5 rows\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.559553Z","iopub.execute_input":"2023-01-05T10:09:53.559917Z","iopub.status.idle":"2023-01-05T10:09:53.574946Z","shell.execute_reply.started":"2023-01-05T10:09:53.559883Z","shell.execute_reply":"2023-01-05T10:09:53.573958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the shape of the data set\nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.576324Z","iopub.execute_input":"2023-01-05T10:09:53.576955Z","iopub.status.idle":"2023-01-05T10:09:53.584948Z","shell.execute_reply.started":"2023-01-05T10:09:53.576911Z","shell.execute_reply":"2023-01-05T10:09:53.583919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print first 5 rows\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.586527Z","iopub.execute_input":"2023-01-05T10:09:53.587142Z","iopub.status.idle":"2023-01-05T10:09:53.599861Z","shell.execute_reply.started":"2023-01-05T10:09:53.587055Z","shell.execute_reply":"2023-01-05T10:09:53.598775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the shape of the data set\nprint(df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.601408Z","iopub.execute_input":"2023-01-05T10:09:53.602308Z","iopub.status.idle":"2023-01-05T10:09:53.610138Z","shell.execute_reply.started":"2023-01-05T10:09:53.602263Z","shell.execute_reply":"2023-01-05T10:09:53.609058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assessing","metadata":{}},{"cell_type":"markdown","source":"## Train dataset","metadata":{"execution":{"iopub.execute_input":"2022-12-02T07:56:50.581818Z","iopub.status.busy":"2022-12-02T07:56:50.581537Z","iopub.status.idle":"2022-12-02T07:56:50.594699Z","shell.execute_reply":"2022-12-02T07:56:50.593728Z","shell.execute_reply.started":"2022-12-02T07:56:50.58177Z"}}},{"cell_type":"code","source":"#check if the data is balanced or not\ndf_train.Emotion.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.611764Z","iopub.execute_input":"2023-01-05T10:09:53.612551Z","iopub.status.idle":"2023-01-05T10:09:53.623784Z","shell.execute_reply.started":"2023-01-05T10:09:53.612509Z","shell.execute_reply":"2023-01-05T10:09:53.622561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check if the data is balanced or not\ndf_train.Emotion.value_counts() / df_train.shape[0] *100","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.628444Z","iopub.execute_input":"2023-01-05T10:09:53.62915Z","iopub.status.idle":"2023-01-05T10:09:53.640209Z","shell.execute_reply.started":"2023-01-05T10:09:53.629113Z","shell.execute_reply":"2023-01-05T10:09:53.639099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(x='Emotion', data=df_train);","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.641892Z","iopub.execute_input":"2023-01-05T10:09:53.642687Z","iopub.status.idle":"2023-01-05T10:09:53.976859Z","shell.execute_reply.started":"2023-01-05T10:09:53.642642Z","shell.execute_reply":"2023-01-05T10:09:53.975068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is imbalanced so, we may try to balance it but also, the emotions that we interested in are the most repeated emotion so, till now we can keep the data as it is","metadata":{}},{"cell_type":"code","source":"#print the number of null values in each column\ndf_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:53.980729Z","iopub.execute_input":"2023-01-05T10:09:53.984527Z","iopub.status.idle":"2023-01-05T10:09:54.000615Z","shell.execute_reply.started":"2023-01-05T10:09:53.984473Z","shell.execute_reply":"2023-01-05T10:09:53.999494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data doesn't contains any null values","metadata":{}},{"cell_type":"code","source":"#print the number of duplicated values \ndf_train.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.00456Z","iopub.execute_input":"2023-01-05T10:09:54.00697Z","iopub.status.idle":"2023-01-05T10:09:54.02812Z","shell.execute_reply.started":"2023-01-05T10:09:54.006932Z","shell.execute_reply":"2023-01-05T10:09:54.027301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is just one row which is duplicated but we need to check also if there are rows having the same text but different emotions","metadata":{}},{"cell_type":"code","source":"#removing duplicated values\nindex = df_train[df_train.duplicated() == True].index\ndf_train.drop(index, axis = 0, inplace = True)\ndf_train.reset_index(inplace=True, drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.031808Z","iopub.execute_input":"2023-01-05T10:09:54.034519Z","iopub.status.idle":"2023-01-05T10:09:54.053943Z","shell.execute_reply.started":"2023-01-05T10:09:54.034469Z","shell.execute_reply":"2023-01-05T10:09:54.053047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the rows which are duplicated (duplicated in the text but with different emotions)\ndf_train[df_train['Text'].duplicated() == True]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.057793Z","iopub.execute_input":"2023-01-05T10:09:54.058261Z","iopub.status.idle":"2023-01-05T10:09:54.080663Z","shell.execute_reply.started":"2023-01-05T10:09:54.058227Z","shell.execute_reply":"2023-01-05T10:09:54.079869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print some of those rows to check\ndf_train[df_train['Text'] == df_train.iloc[7623]['Text']]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.084354Z","iopub.execute_input":"2023-01-05T10:09:54.08683Z","iopub.status.idle":"2023-01-05T10:09:54.102754Z","shell.execute_reply.started":"2023-01-05T10:09:54.086794Z","shell.execute_reply":"2023-01-05T10:09:54.101835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['Text'] == df_train.iloc[14313]['Text']]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.106334Z","iopub.execute_input":"2023-01-05T10:09:54.108766Z","iopub.status.idle":"2023-01-05T10:09:54.124849Z","shell.execute_reply.started":"2023-01-05T10:09:54.108731Z","shell.execute_reply":"2023-01-05T10:09:54.124053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['Text'] == df_train.iloc[13879]['Text']]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.128574Z","iopub.execute_input":"2023-01-05T10:09:54.131002Z","iopub.status.idle":"2023-01-05T10:09:54.146983Z","shell.execute_reply.started":"2023-01-05T10:09:54.130966Z","shell.execute_reply":"2023-01-05T10:09:54.146155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing duplicated text \nindex = df_train[df_train['Text'].duplicated() == True].index\ndf_train.drop(index, axis = 0, inplace = True)\ndf_train.reset_index(inplace=True, drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.150496Z","iopub.execute_input":"2023-01-05T10:09:54.153065Z","iopub.status.idle":"2023-01-05T10:09:54.164005Z","shell.execute_reply.started":"2023-01-05T10:09:54.153024Z","shell.execute_reply":"2023-01-05T10:09:54.162824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Count the number of stopwords in the data\ntemp =df_train.copy()\nstop_words = set(stopwords.words(\"english\"))\ntemp['stop_words'] = temp['Text'].apply(lambda x: len(set(x.split()) & set(stop_words)))\ntemp.stop_words.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.168264Z","iopub.execute_input":"2023-01-05T10:09:54.170905Z","iopub.status.idle":"2023-01-05T10:09:54.329762Z","shell.execute_reply.started":"2023-01-05T10:09:54.170855Z","shell.execute_reply":"2023-01-05T10:09:54.328952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains alot of stopwords (some rows contains more than 25 stopword!) so, we need to take care when we remove them as some rows may become empty ","metadata":{}},{"cell_type":"code","source":"#distribution of stopwords visually\ntemp['stop_words'].plot(kind= 'hist')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.333655Z","iopub.execute_input":"2023-01-05T10:09:54.336164Z","iopub.status.idle":"2023-01-05T10:09:54.62232Z","shell.execute_reply.started":"2023-01-05T10:09:54.336126Z","shell.execute_reply":"2023-01-05T10:09:54.621388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test dataset","metadata":{}},{"cell_type":"code","source":"#check if the data is balanced or not\ndf_test.Emotion.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.623834Z","iopub.execute_input":"2023-01-05T10:09:54.624394Z","iopub.status.idle":"2023-01-05T10:09:54.632104Z","shell.execute_reply.started":"2023-01-05T10:09:54.624121Z","shell.execute_reply":"2023-01-05T10:09:54.631037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(x='Emotion', data=df_test);\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.63369Z","iopub.execute_input":"2023-01-05T10:09:54.634696Z","iopub.status.idle":"2023-01-05T10:09:54.854002Z","shell.execute_reply.started":"2023-01-05T10:09:54.63465Z","shell.execute_reply":"2023-01-05T10:09:54.853021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the number of null values in each column\ndf_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.860094Z","iopub.execute_input":"2023-01-05T10:09:54.860871Z","iopub.status.idle":"2023-01-05T10:09:54.870549Z","shell.execute_reply.started":"2023-01-05T10:09:54.860834Z","shell.execute_reply":"2023-01-05T10:09:54.869658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the number of duplicated values \ndf_test.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.872531Z","iopub.execute_input":"2023-01-05T10:09:54.873531Z","iopub.status.idle":"2023-01-05T10:09:54.883866Z","shell.execute_reply.started":"2023-01-05T10:09:54.873489Z","shell.execute_reply":"2023-01-05T10:09:54.882659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#print the rows which are duplicated\ndf_test[df_test['Text'].duplicated() == True]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.885661Z","iopub.execute_input":"2023-01-05T10:09:54.886161Z","iopub.status.idle":"2023-01-05T10:09:54.897041Z","shell.execute_reply.started":"2023-01-05T10:09:54.886125Z","shell.execute_reply":"2023-01-05T10:09:54.895631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Count the number of stopwords in the data\ntemp =df_test.copy()\ntemp['stop_words'] = temp['Text'].apply(lambda x: len(set(x.split()) & set(stop_words)))\ntemp.stop_words.value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.898246Z","iopub.execute_input":"2023-01-05T10:09:54.898528Z","iopub.status.idle":"2023-01-05T10:09:54.923974Z","shell.execute_reply.started":"2023-01-05T10:09:54.898491Z","shell.execute_reply":"2023-01-05T10:09:54.922899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.3)\ntemp['stop_words'].plot(kind= 'hist')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:54.925538Z","iopub.execute_input":"2023-01-05T10:09:54.925886Z","iopub.status.idle":"2023-01-05T10:09:55.187831Z","shell.execute_reply.started":"2023-01-05T10:09:54.925852Z","shell.execute_reply":"2023-01-05T10:09:55.18697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation dataset","metadata":{}},{"cell_type":"code","source":"#check if the data is balanced or not\ndf_val.Emotion.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.189282Z","iopub.execute_input":"2023-01-05T10:09:55.189659Z","iopub.status.idle":"2023-01-05T10:09:55.198579Z","shell.execute_reply.started":"2023-01-05T10:09:55.189623Z","shell.execute_reply":"2023-01-05T10:09:55.197542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(x='Emotion', data=df_val);","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.201605Z","iopub.execute_input":"2023-01-05T10:09:55.201846Z","iopub.status.idle":"2023-01-05T10:09:55.618527Z","shell.execute_reply.started":"2023-01-05T10:09:55.201822Z","shell.execute_reply":"2023-01-05T10:09:55.617572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the number of null values in each column\ndf_val.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.620053Z","iopub.execute_input":"2023-01-05T10:09:55.620413Z","iopub.status.idle":"2023-01-05T10:09:55.631432Z","shell.execute_reply.started":"2023-01-05T10:09:55.620377Z","shell.execute_reply":"2023-01-05T10:09:55.630638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the number of duplicated values \ndf_val.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.632932Z","iopub.execute_input":"2023-01-05T10:09:55.633676Z","iopub.status.idle":"2023-01-05T10:09:55.643512Z","shell.execute_reply.started":"2023-01-05T10:09:55.633639Z","shell.execute_reply":"2023-01-05T10:09:55.642004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the rows which are duplicated\ndf_val[df_val['Text'].duplicated() == True]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.645201Z","iopub.execute_input":"2023-01-05T10:09:55.645626Z","iopub.status.idle":"2023-01-05T10:09:55.658246Z","shell.execute_reply.started":"2023-01-05T10:09:55.645592Z","shell.execute_reply":"2023-01-05T10:09:55.657028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[df_val['Text'] == df_val.iloc[603]['Text']]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.659744Z","iopub.execute_input":"2023-01-05T10:09:55.660164Z","iopub.status.idle":"2023-01-05T10:09:55.671763Z","shell.execute_reply.started":"2023-01-05T10:09:55.660126Z","shell.execute_reply":"2023-01-05T10:09:55.670646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val[df_val['Text'] == df_val.iloc[1993]['Text']]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.673469Z","iopub.execute_input":"2023-01-05T10:09:55.673885Z","iopub.status.idle":"2023-01-05T10:09:55.685653Z","shell.execute_reply.started":"2023-01-05T10:09:55.673852Z","shell.execute_reply":"2023-01-05T10:09:55.68444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing duplicated text\nindex = df_val[df_val['Text'].duplicated() == True].index\ndf_val.drop(index, axis = 0, inplace = True)\ndf_val.reset_index(inplace=True, drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.687263Z","iopub.execute_input":"2023-01-05T10:09:55.687616Z","iopub.status.idle":"2023-01-05T10:09:55.695224Z","shell.execute_reply.started":"2023-01-05T10:09:55.687583Z","shell.execute_reply":"2023-01-05T10:09:55.694326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Count the number of stopwords in the data\ntemp =df_val.copy()\ntemp['stop_words'] = temp['Text'].apply(lambda x: len(set(x.split()) & set(stop_words)))\ntemp.stop_words.value_counts()[:10]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.696313Z","iopub.execute_input":"2023-01-05T10:09:55.697324Z","iopub.status.idle":"2023-01-05T10:09:55.720223Z","shell.execute_reply.started":"2023-01-05T10:09:55.697279Z","shell.execute_reply":"2023-01-05T10:09:55.719252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.3)\ntemp['stop_words'].plot(kind= 'hist');","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.721268Z","iopub.execute_input":"2023-01-05T10:09:55.722037Z","iopub.status.idle":"2023-01-05T10:09:55.971404Z","shell.execute_reply.started":"2023-01-05T10:09:55.72201Z","shell.execute_reply":"2023-01-05T10:09:55.970403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare rows of the datasets","metadata":{}},{"cell_type":"markdown","source":"check that there is no data leakage","metadata":{}},{"cell_type":"code","source":"def dataframe_difference(df1, df2, which=None):\n    \"\"\"Find rows which are different between two DataFrames.\"\"\"\n    \n    # Combine the two DataFrames using a merge operation, with the\n    # indicator parameter set to True. This adds a column called _merge\n    # to the resulting DataFrame, which indicates the source of each row.\n    comparison_df = df1.merge(\n        df2,\n        indicator=True,\n        how='outer'\n    )\n    \n    # Filter the merged DataFrame based on the value of _merge. If which\n    # is not specified, return all rows where _merge is not 'both'.\n    # Otherwise, return all rows where _merge has the specified value\n    if which is None:\n        diff_df = comparison_df[comparison_df['_merge'] != 'both']\n    else:\n        diff_df = comparison_df[comparison_df['_merge'] == which]\n        \n    # Return the filtered DataFrame\n    return diff_df","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.974863Z","iopub.execute_input":"2023-01-05T10:09:55.975135Z","iopub.status.idle":"2023-01-05T10:09:55.981163Z","shell.execute_reply.started":"2023-01-05T10:09:55.975109Z","shell.execute_reply":"2023-01-05T10:09:55.979354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_difference(df_train, df_test, which='both')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:55.982729Z","iopub.execute_input":"2023-01-05T10:09:55.983424Z","iopub.status.idle":"2023-01-05T10:09:56.01836Z","shell.execute_reply.started":"2023-01-05T10:09:55.983374Z","shell.execute_reply":"2023-01-05T10:09:56.017495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_difference(df_train, df_val, which='both')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:56.019838Z","iopub.execute_input":"2023-01-05T10:09:56.020174Z","iopub.status.idle":"2023-01-05T10:09:56.049856Z","shell.execute_reply.started":"2023-01-05T10:09:56.020137Z","shell.execute_reply":"2023-01-05T10:09:56.049039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_difference(df_val, df_test, which='both')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:56.050907Z","iopub.execute_input":"2023-01-05T10:09:56.051154Z","iopub.status.idle":"2023-01-05T10:09:56.072201Z","shell.execute_reply.started":"2023-01-05T10:09:56.051131Z","shell.execute_reply":"2023-01-05T10:09:56.071213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning","metadata":{}},{"cell_type":"code","source":"def lemmatization(text):\n    lemmatizer= WordNetLemmatizer()\n\n    text = text.split()\n\n    text=[lemmatizer.lemmatize(y) for y in text]\n    \n    return \" \" .join(text)\n\ndef remove_stop_words(text):\n\n    Text=[i for i in str(text).split() if i not in stop_words]\n    return \" \".join(Text)\n\ndef Removing_numbers(text):\n    text=''.join([i for i in text if not i.isdigit()])\n    return text\n\ndef lower_case(text):\n    \n    text = text.split()\n\n    text=[y.lower() for y in text]\n    \n    return \" \" .join(text)\n\ndef Removing_punctuations(text):\n    ## Remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n    text = text.replace('؛',\"\", )\n    \n    ## remove extra whitespace\n    text = re.sub('\\s+', ' ', text)\n    text =  \" \".join(text.split())\n    return text.strip()\n\ndef Removing_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_small_sentences(df):\n    for i in range(len(df)):\n        if len(df.text.iloc[i].split()) < 3:\n            df.text.iloc[i] = np.nan\n            \ndef normalize_text(df):\n    df.Text=df.Text.apply(lambda text : lower_case(text))\n    df.Text=df.Text.apply(lambda text : remove_stop_words(text))\n    df.Text=df.Text.apply(lambda text : Removing_numbers(text))\n    df.Text=df.Text.apply(lambda text : Removing_punctuations(text))\n    df.Text=df.Text.apply(lambda text : Removing_urls(text))\n    df.Text=df.Text.apply(lambda text : lemmatization(text))\n    return df\n\ndef normalized_sentence(sentence):\n    sentence= lower_case(sentence)\n    sentence= remove_stop_words(sentence)\n    sentence= Removing_numbers(sentence)\n    sentence= Removing_punctuations(sentence)\n    sentence= Removing_urls(sentence)\n    sentence= lemmatization(sentence)\n    return sentence\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:09:56.074027Z","iopub.execute_input":"2023-01-05T10:09:56.074361Z","iopub.status.idle":"2023-01-05T10:09:56.087635Z","shell.execute_reply.started":"2023-01-05T10:09:56.074328Z","shell.execute_reply":"2023-01-05T10:09:56.086445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalized_sentence(\"My Name is Mohamed. @Tweets,  plays 2022  Egypt_\")\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:02.376175Z","iopub.execute_input":"2023-01-05T10:11:02.376585Z","iopub.status.idle":"2023-01-05T10:11:04.034699Z","shell.execute_reply.started":"2023-01-05T10:11:02.376542Z","shell.execute_reply":"2023-01-05T10:11:04.033722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train= normalize_text(df_train)\ndf_test= normalize_text(df_test)\ndf_val= normalize_text(df_val)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:07.323879Z","iopub.execute_input":"2023-01-05T10:11:07.3265Z","iopub.status.idle":"2023-01-05T10:11:08.664858Z","shell.execute_reply.started":"2023-01-05T10:11:07.326446Z","shell.execute_reply":"2023-01-05T10:11:08.663886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF\nThe word counts suffer some issues: most frequent words are usually not important (like stop words), while they take high focus/count.\n\nTFIDF (Term Frequency - Inverse Document) is a way to adjust those counts:\n\nTF: #mentions within a document\n\nIDF: #mentions across all docs \n\nSo it gives higher importance to rare words across all docs (IDF++, TFIDF--), while it emphasyses on words appearing mostly in THIS doc (TF++, TFIDF--).\n\nIf a word appearing only in the current doc/sentence, it has TFIDF=1. If a word appears in all docs/sents but not the current one, it has TFIDF=0. If a word (stop word for example) appearing a lot in the current doc/sent and also in ALL others, it will have high TF (count) and much higher IDF (discount), so low TFIDF overall.","metadata":{}},{"cell_type":"markdown","source":"We will start by baseline models","metadata":{}},{"cell_type":"markdown","source":"**Creating a pipline using tf-idf for words embedding and different models.**\n","metadata":{}},{"cell_type":"code","source":"#Preprocess text\nX_train = df_train['Text'].values\ny_train = df_train['Emotion'].values\n\nX_test = df_test['Text'].values\ny_test = df_test['Emotion'].values\n\nX_val = df_val['Text'].values\ny_val = df_val['Emotion'].values","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:08.667044Z","iopub.execute_input":"2023-01-05T10:11:08.667751Z","iopub.status.idle":"2023-01-05T10:11:08.674691Z","shell.execute_reply.started":"2023-01-05T10:11:08.667713Z","shell.execute_reply":"2023-01-05T10:11:08.67344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, data, targets):\n    \"\"\"\n    Train a model on the given data and targets.\n    \n    Parameters:\n    model (sklearn model): The model to be trained.\n    data (list of str): The input data.\n    targets (list of str): The targets.\n    \n    Returns:\n    Pipeline: The trained model as a Pipeline object.\n    \"\"\"\n    # Create a Pipeline object with a TfidfVectorizer and the given model\n    text_clf = Pipeline([('vect',TfidfVectorizer()),\n                         ('clf', model)])\n    # Fit the model on the data and targets\n    text_clf.fit(data, targets)\n    return text_clf","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:08.676039Z","iopub.execute_input":"2023-01-05T10:11:08.676594Z","iopub.status.idle":"2023-01-05T10:11:08.68658Z","shell.execute_reply.started":"2023-01-05T10:11:08.676559Z","shell.execute_reply":"2023-01-05T10:11:08.685724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_F1(trained_model,X,y):\n    \"\"\"\n    Get the F1 score for the given model on the given data and targets.\n    \n    Parameters:\n    trained_model (sklearn model): The trained model.\n    X (list of str): The input data.\n    y (list of str): The targets.\n    \n    Returns:\n    array: The F1 score for each class.\n    \"\"\"\n    # Make predictions on the input data using the trained model\n    predicted=trained_model.predict(X)\n    # Calculate the F1 score for the predictions\n    f1=f1_score(y,predicted, average=None)\n    # Return the F1 score\n    return f1","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:08.690059Z","iopub.execute_input":"2023-01-05T10:11:08.690396Z","iopub.status.idle":"2023-01-05T10:11:08.698144Z","shell.execute_reply.started":"2023-01-05T10:11:08.690363Z","shell.execute_reply":"2023-01-05T10:11:08.696823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Logistic Regression model on the Training set**\n","metadata":{}},{"cell_type":"code","source":"#Train the model with the training data\nlog_reg = train_model(LogisticRegression(solver='liblinear',random_state = 0), X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:08.700326Z","iopub.execute_input":"2023-01-05T10:11:08.70091Z","iopub.status.idle":"2023-01-05T10:11:09.480896Z","shell.execute_reply.started":"2023-01-05T10:11:08.700849Z","shell.execute_reply":"2023-01-05T10:11:09.479578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make a single prediction\ny_pred=log_reg.predict(['Happy'])\ny_pred","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:09.482835Z","iopub.execute_input":"2023-01-05T10:11:09.483608Z","iopub.status.idle":"2023-01-05T10:11:09.49367Z","shell.execute_reply.started":"2023-01-05T10:11:09.483558Z","shell.execute_reply":"2023-01-05T10:11:09.492466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test the model with the test data\ny_pred=log_reg.predict(X_test)\n\n#calculate the accuracy\nlog_reg_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', log_reg_accuracy,'\\n')\n\n#calculate the F1 score\nf1_Score = get_F1(log_reg,X_test,y_test)\npd.DataFrame(f1_Score, index=df_train.Emotion.unique(), columns=['F1 score'])","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:09.495464Z","iopub.execute_input":"2023-01-05T10:11:09.496203Z","iopub.status.idle":"2023-01-05T10:11:09.628897Z","shell.execute_reply.started":"2023-01-05T10:11:09.496164Z","shell.execute_reply":"2023-01-05T10:11:09.627742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Classification Report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:09.630758Z","iopub.execute_input":"2023-01-05T10:11:09.63102Z","iopub.status.idle":"2023-01-05T10:11:09.680051Z","shell.execute_reply.started":"2023-01-05T10:11:09.630995Z","shell.execute_reply":"2023-01-05T10:11:09.679035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Decision Tree model on the Training set**","metadata":{}},{"cell_type":"code","source":"#Train the model with the training data\nDT = train_model(DecisionTreeClassifier(random_state = 0), X_train, y_train)\n\n#test the model with the test data\ny_pred=DT.predict(X_test)\n\n#calculate the accuracy\nDT_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', DT_accuracy,'\\n')\n\n#calculate the F1 score\nf1_Score = get_F1(DT,X_test,y_test)\npd.DataFrame(f1_Score, index=df_train.Emotion.unique(), columns=['F1 score']) ","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:09.682902Z","iopub.execute_input":"2023-01-05T10:11:09.683165Z","iopub.status.idle":"2023-01-05T10:11:13.620461Z","shell.execute_reply.started":"2023-01-05T10:11:09.68314Z","shell.execute_reply":"2023-01-05T10:11:13.618985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Classification Report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:13.625218Z","iopub.execute_input":"2023-01-05T10:11:13.625609Z","iopub.status.idle":"2023-01-05T10:11:13.674383Z","shell.execute_reply.started":"2023-01-05T10:11:13.625572Z","shell.execute_reply":"2023-01-05T10:11:13.673504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Support Vector Machine model on the Training set**\n","metadata":{}},{"cell_type":"code","source":"#Train the model with the training data\nSVM = train_model(SVC(random_state = 0), X_train, y_train)\n\n#test the model with the test data\ny_pred=SVM.predict(X_test)\n\n#calculate the accuracy\nSVM_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', SVM_accuracy,'\\n')\n\n#calculate the F1 score\nf1_Score = get_F1(SVM,X_test,y_test)\npd.DataFrame(f1_Score, index=df_train.Emotion.unique(), columns=['F1 score']) \n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:11:13.675804Z","iopub.execute_input":"2023-01-05T10:11:13.676153Z","iopub.status.idle":"2023-01-05T10:12:21.751364Z","shell.execute_reply.started":"2023-01-05T10:11:13.676119Z","shell.execute_reply":"2023-01-05T10:12:21.750335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Classification Report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:21.752811Z","iopub.execute_input":"2023-01-05T10:12:21.753491Z","iopub.status.idle":"2023-01-05T10:12:21.803906Z","shell.execute_reply.started":"2023-01-05T10:12:21.753433Z","shell.execute_reply":"2023-01-05T10:12:21.80274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the Random Forest model on the Training set**","metadata":{}},{"cell_type":"code","source":"#Train the model with the training data\nRF = train_model(RandomForestClassifier(random_state = 0), X_train, y_train)\n\n#test the model with the test data\ny_pred=RF.predict(X_test)\n\n#calculate the accuracy\nRF_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', RF_accuracy,'\\n')\n\n#calculate the F1 score\nf1_Score = get_F1(RF, X_test, y_test)\npd.DataFrame(f1_Score, index=df_train.Emotion.unique(), columns=['F1 score']) ","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:21.807118Z","iopub.execute_input":"2023-01-05T10:12:21.807381Z","iopub.status.idle":"2023-01-05T10:12:37.653137Z","shell.execute_reply.started":"2023-01-05T10:12:21.807356Z","shell.execute_reply":"2023-01-05T10:12:37.652005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Classification Report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:37.654558Z","iopub.execute_input":"2023-01-05T10:12:37.655007Z","iopub.status.idle":"2023-01-05T10:12:37.702134Z","shell.execute_reply.started":"2023-01-05T10:12:37.654971Z","shell.execute_reply":"2023-01-05T10:12:37.701186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree','Support Vector Machine','Random Forest'],\n    'Accuracy': [log_reg_accuracy.round(2), DT_accuracy.round(2), SVM_accuracy.round(2), RF_accuracy.round(2)]})\n\nmodels.sort_values(by='Accuracy', ascending=False).reset_index().drop(['index'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:37.703396Z","iopub.execute_input":"2023-01-05T10:12:37.703751Z","iopub.status.idle":"2023-01-05T10:12:37.723893Z","shell.execute_reply.started":"2023-01-05T10:12:37.703718Z","shell.execute_reply":"2023-01-05T10:12:37.722892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lime\n\nLIME, the acronym for local interpretable model-agnostic explanations, is a technique that approximates any black box machine learning model with a local, interpretable model to explain each individual prediction\n\nwe need which words contributed the most in the predicition","metadata":{}},{"cell_type":"code","source":"#c_LR = make_pipeline(tfidf, clf)\nexplainer_LR = LimeTextExplainer(class_names=RF.classes_)\nidx  = 15\nprint(\"Actual Text : \", X_test[idx])\nprint(\"Prediction : \", RF.predict(X_test)[idx])\nprint(\"Actual :     \", y_test[idx])\nexp = explainer_LR.explain_instance(X_test[idx], RF.predict_proba,top_labels=5)\nexp.show_in_notebook()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:37.725576Z","iopub.execute_input":"2023-01-05T10:12:37.726297Z","iopub.status.idle":"2023-01-05T10:12:38.963397Z","shell.execute_reply.started":"2023-01-05T10:12:37.726254Z","shell.execute_reply":"2023-01-05T10:12:38.962527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Better way?\nWord embedding using this glove and use a sequance model ","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"code","source":"#Splitting the text from the labels\nX_train = df_train['Text']\ny_train = df_train['Emotion']\n\nX_test = df_test['Text']\ny_test = df_test['Emotion']\n\nX_val = df_val['Text']\ny_val = df_val['Emotion']","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:38.964778Z","iopub.execute_input":"2023-01-05T10:12:38.96584Z","iopub.status.idle":"2023-01-05T10:12:38.974982Z","shell.execute_reply.started":"2023-01-05T10:12:38.965784Z","shell.execute_reply":"2023-01-05T10:12:38.973972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode labels\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)\ny_val = le.transform(y_val)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:38.976451Z","iopub.execute_input":"2023-01-05T10:12:38.978322Z","iopub.status.idle":"2023-01-05T10:12:39.007468Z","shell.execute_reply.started":"2023-01-05T10:12:38.978286Z","shell.execute_reply":"2023-01-05T10:12:39.006531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the labels after encoding \nprint(set(y_train))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.008789Z","iopub.execute_input":"2023-01-05T10:12:39.009968Z","iopub.status.idle":"2023-01-05T10:12:39.021194Z","shell.execute_reply.started":"2023-01-05T10:12:39.009932Z","shell.execute_reply":"2023-01-05T10:12:39.020358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert the class vector (integers) to binary class matrix\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.022968Z","iopub.execute_input":"2023-01-05T10:12:39.024764Z","iopub.status.idle":"2023-01-05T10:12:39.034271Z","shell.execute_reply.started":"2023-01-05T10:12:39.024729Z","shell.execute_reply":"2023-01-05T10:12:39.033206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.036011Z","iopub.execute_input":"2023-01-05T10:12:39.037903Z","iopub.status.idle":"2023-01-05T10:12:39.048257Z","shell.execute_reply.started":"2023-01-05T10:12:39.037869Z","shell.execute_reply":"2023-01-05T10:12:39.047076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing","metadata":{}},{"cell_type":"code","source":"# Tokenize words\ntokenizer = Tokenizer(oov_token='UNK')\ntokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.050251Z","iopub.execute_input":"2023-01-05T10:12:39.053114Z","iopub.status.idle":"2023-01-05T10:12:39.491565Z","shell.execute_reply.started":"2023-01-05T10:12:39.05308Z","shell.execute_reply":"2023-01-05T10:12:39.490609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word index: A dictionary of words and their uniquely assigned integers.\n#tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.493033Z","iopub.execute_input":"2023-01-05T10:12:39.493399Z","iopub.status.idle":"2023-01-05T10:12:39.500213Z","shell.execute_reply.started":"2023-01-05T10:12:39.49336Z","shell.execute_reply":"2023-01-05T10:12:39.497504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word counts: A dictionary of words and their counts.\n#tokenizer.word_counts","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.502544Z","iopub.execute_input":"2023-01-05T10:12:39.503208Z","iopub.status.idle":"2023-01-05T10:12:39.508561Z","shell.execute_reply.started":"2023-01-05T10:12:39.503173Z","shell.execute_reply":"2023-01-05T10:12:39.507616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#document count: A dictionary of words and how many documents each appeared in.\n#in this dataset the output will be the number of rows\ntokenizer.document_count","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.509888Z","iopub.execute_input":"2023-01-05T10:12:39.510409Z","iopub.status.idle":"2023-01-05T10:12:39.520532Z","shell.execute_reply.started":"2023-01-05T10:12:39.510374Z","shell.execute_reply":"2023-01-05T10:12:39.519467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word docs: An integer count of the total number of documents(or rows) that were used to fit the Tokenizer.\n#tokenizer.word_docs","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.522146Z","iopub.execute_input":"2023-01-05T10:12:39.522653Z","iopub.status.idle":"2023-01-05T10:12:39.529548Z","shell.execute_reply.started":"2023-01-05T10:12:39.522617Z","shell.execute_reply":"2023-01-05T10:12:39.528539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the index of a single word\ntokenizer.word_index['towards']","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.531039Z","iopub.execute_input":"2023-01-05T10:12:39.531379Z","iopub.status.idle":"2023-01-05T10:12:39.541452Z","shell.execute_reply.started":"2023-01-05T10:12:39.531345Z","shell.execute_reply":"2023-01-05T10:12:39.540524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting a single sentence to list of indexes\ntokenizer.texts_to_sequences(X_train[0].split())","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.548924Z","iopub.execute_input":"2023-01-05T10:12:39.549184Z","iopub.status.idle":"2023-01-05T10:12:39.555619Z","shell.execute_reply.started":"2023-01-05T10:12:39.549161Z","shell.execute_reply":"2023-01-05T10:12:39.554677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert the list of indexes into a matrix of ones and zeros (BOW)\ntokenizer.texts_to_matrix(X_train[0].split())","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.557129Z","iopub.execute_input":"2023-01-05T10:12:39.558358Z","iopub.status.idle":"2023-01-05T10:12:39.56661Z","shell.execute_reply.started":"2023-01-05T10:12:39.55832Z","shell.execute_reply":"2023-01-05T10:12:39.565514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the sentence contains three words and the size of the vocabulary is 14325\ntokenizer.texts_to_matrix(X_train[0].split()).shape","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.568172Z","iopub.execute_input":"2023-01-05T10:12:39.568972Z","iopub.status.idle":"2023-01-05T10:12:39.579073Z","shell.execute_reply.started":"2023-01-05T10:12:39.568937Z","shell.execute_reply":"2023-01-05T10:12:39.578011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nsequences_val = tokenizer.texts_to_sequences(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.580444Z","iopub.execute_input":"2023-01-05T10:12:39.581585Z","iopub.status.idle":"2023-01-05T10:12:39.829555Z","shell.execute_reply.started":"2023-01-05T10:12:39.581543Z","shell.execute_reply":"2023-01-05T10:12:39.828535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the sentence after converting them to indexes\n#sequences_train","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.830863Z","iopub.execute_input":"2023-01-05T10:12:39.831216Z","iopub.status.idle":"2023-01-05T10:12:39.837279Z","shell.execute_reply.started":"2023-01-05T10:12:39.83118Z","shell.execute_reply":"2023-01-05T10:12:39.83543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding\n\nThe vectors we obtain are generally not of equal lengths\n\nFor that, we might need to pad the sequences to max len. ","metadata":{}},{"cell_type":"code","source":"df_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.838961Z","iopub.execute_input":"2023-01-05T10:12:39.841032Z","iopub.status.idle":"2023-01-05T10:12:39.847969Z","shell.execute_reply.started":"2023-01-05T10:12:39.840996Z","shell.execute_reply":"2023-01-05T10:12:39.846855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max([len(t) for t in df_train['Text']])\nmaxlen","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.849312Z","iopub.execute_input":"2023-01-05T10:12:39.850113Z","iopub.status.idle":"2023-01-05T10:12:39.864752Z","shell.execute_reply.started":"2023-01-05T10:12:39.850075Z","shell.execute_reply":"2023-01-05T10:12:39.863877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pad_sequences(sequences_train, maxlen=229, truncating='pre')\nX_test = pad_sequences(sequences_test, maxlen=229, truncating='pre')\nX_val = pad_sequences(sequences_val, maxlen=229, truncating='pre')\n\nvocabSize = len(tokenizer.index_word) + 1\nprint(f\"Vocabulary size = {vocabSize}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.86752Z","iopub.execute_input":"2023-01-05T10:12:39.868604Z","iopub.status.idle":"2023-01-05T10:12:39.930522Z","shell.execute_reply.started":"2023-01-05T10:12:39.868559Z","shell.execute_reply":"2023-01-05T10:12:39.929549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#before\nsequences_train[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.931958Z","iopub.execute_input":"2023-01-05T10:12:39.932313Z","iopub.status.idle":"2023-01-05T10:12:39.93955Z","shell.execute_reply.started":"2023-01-05T10:12:39.932278Z","shell.execute_reply":"2023-01-05T10:12:39.937664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after\nX_train[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.941045Z","iopub.execute_input":"2023-01-05T10:12:39.942622Z","iopub.status.idle":"2023-01-05T10:12:39.950034Z","shell.execute_reply.started":"2023-01-05T10:12:39.942578Z","shell.execute_reply":"2023-01-05T10:12:39.948968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Embedding","metadata":{}},{"cell_type":"markdown","source":"**This code defines the following variables:**\n\n**path_to_glove_file**: The file path to the GloVe word vectors file.\n\n**num_tokens**: The size of the vocabulary. This is typically the number of unique words in the dataset.\n\n**embedding_dim**: The dimensionality of the word vectors. This is usually set to a fixed value (e.g., 200, 300, etc.) depending on the size of the word vectors file that is being used.\n\n**hits**: A counter for the number of words that are found in the embeddings_index dictionary.\n\n**misses**: A counter for the number of words that are not found in the embeddings_index dictionary.\n\n**embeddings_index**: A dictionary that will store the word vectors, with the words as keys and the word vectors as values.","metadata":{}},{"cell_type":"code","source":"# Read GloVE embeddings\n\npath_to_glove_file = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\nnum_tokens = vocabSize\nembedding_dim = 200 #latent factors or features  \nhits = 0\nmisses = 0\nembeddings_index = {}","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:06:15.441637Z","iopub.execute_input":"2023-01-05T11:06:15.442524Z","iopub.status.idle":"2023-01-05T11:06:15.699996Z","shell.execute_reply.started":"2023-01-05T11:06:15.442464Z","shell.execute_reply":"2023-01-05T11:06:15.698758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will intialize an embedding matrix with all zero values and then looping through the vocabulary (as defined by the tokenizer object) to assign word vectors to the corresponding rows of the embedding matrix. The word_index attribute of the tokenizer object is a dictionary that maps words to their indices in the vocabulary.\n\nThe embedding matrix will have a size of num_tokens rows and embedding_dim columns, where num_tokens is the size of the vocabulary and embedding_dim is the dimensionality of the word vectors.\n\nFor each word in the vocabulary, the code looks up the corresponding word vector in the embeddings_index dictionary and assigns it to the corresponding row of the embedding matrix. If the word is not found in the embeddings_index dictionary (i.e., if embedding_vector is None), the code increments the misses counter and the row of the embedding matrix remains all-zeros. If the word is found in the embeddings_index dictionary, the code increments the hits counter and assigns the word vector to the corresponding row of the embedding matrix. Finally, the code prints out the number of words that were converted (hits) and the number that were not found in the embeddings_index dictionary (misses).","metadata":{}},{"cell_type":"code","source":"# Read word vectors\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n# Assign word vectors to our dictionary/vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:39.951525Z","iopub.execute_input":"2023-01-05T10:12:39.952676Z","iopub.status.idle":"2023-01-05T10:12:39.95807Z","shell.execute_reply.started":"2023-01-05T10:12:39.952639Z","shell.execute_reply":"2023-01-05T10:12:39.95695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"**The model consists of several layers:**\n\n1. An Embedding layer that takes in integer-encoded text and looks up the embedding vector for each word in the input. The input length is set to the length of the input sequences, and the weights are initialized with an embedding matrix that has been pre-trained on a large dataset. The embedding layer is not trainable, meaning that the weights of this layer will not be updated during training.\n\n2. Three Bidirectional layers, each of which applies a forward and backward LSTM to the input. LSTMs are a type of recurrent neural network that are able to capture long-term dependencies in sequential data. The dropout and recurrent dropout arguments specify the dropout rate to apply to the inputs and recurrent inputs of the LSTMs, respectively. The return_sequences argument specifies whether the LSTM should return the full sequences of outputs or just the last output of each sequence.\n\n3. A Dense layer that applies a linear transformation to the input to produce the output. The output has 6 units and uses the 'softmax' activation function, which means that it will output a probability distribution over the 6 classes.","metadata":{}},{"cell_type":"code","source":"# Build neural network architecture\n\nadam = Adam(learning_rate=0.005)\n\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:12:58.619119Z","iopub.execute_input":"2023-01-05T10:12:58.61956Z","iopub.status.idle":"2023-01-05T10:13:02.233541Z","shell.execute_reply.started":"2023-01-05T10:12:58.619524Z","shell.execute_reply":"2023-01-05T10:13:02.232616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the summary of a deep learning model built using the Keras functional API. The model consists of several layers, each of which has a specific type and number of parameters.**\n\n1. The first layer is an \"Embedding\" layer, which takes in integer-encoded text and looks up the embedding vector for each word in the input. The output of this layer is a 3D tensor with shape (batch_size, sequence_length, embedding_dim), where batch_size is the number of examples in the batch, sequence_length is the length of the input sequences, and embedding_dim is the size of the embedding vectors. In this case, the input sequences are 229 words long and the embedding vectors are 200 dimensions. The embedding layer has a total of 2,863,600 trainable parameters.\n\n2. The next three layers are \"Bidirectional\" layers, which apply a forward and backward LSTM to the input. LSTMs are a type of recurrent neural network that are able to capture long-term dependencies in sequential data. The output of each bidirectional layer is a 3D tensor with shape (batch_size, sequence_length, units), where units is the number of LSTM units in the layer. In this case, the first bidirectional layer has 512 units, the second has 256 units, and the third has 256 units. Each of these layers has a large number of trainable parameters, as LSTMs are complex models with many internal weights.\n\n3. The final layer is a \"Dense\" layer, which applies a linear transformation to the input to produce the output. In this case, the output has shape (batch_size, 6), indicating that there are 6 classes. The dense layer has a total of 1,542 trainable parameters.\n\nOverall, the model has a total of 4,851,702 trainable parameters and 2,863,600 non-trainable parameters.","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:13:02.235068Z","iopub.execute_input":"2023-01-05T10:13:02.235418Z","iopub.status.idle":"2023-01-05T10:13:03.221465Z","shell.execute_reply.started":"2023-01-05T10:13:02.235381Z","shell.execute_reply":"2023-01-05T10:13:03.220278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to stop the training when the loss starts to increase\ncallback = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=4,\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:13:03.223135Z","iopub.execute_input":"2023-01-05T10:13:03.224786Z","iopub.status.idle":"2023-01-05T10:13:03.23041Z","shell.execute_reply.started":"2023-01-05T10:13:03.22474Z","shell.execute_reply":"2023-01-05T10:13:03.229216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_val, y_val),\n                    verbose=1,\n                    batch_size=256,\n                    epochs=30,\n                    callbacks=[callback]\n                   )","metadata":{"execution":{"iopub.status.busy":"2023-01-05T10:13:03.232094Z","iopub.execute_input":"2023-01-05T10:13:03.232886Z","iopub.status.idle":"2023-01-05T11:05:08.531553Z","shell.execute_reply.started":"2023-01-05T10:13:03.232848Z","shell.execute_reply":"2023-01-05T11:05:08.530537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the overall loss and accuracy\nmodel.evaluate(X_val, y_val, verbose=1) ","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:05:08.533118Z","iopub.execute_input":"2023-01-05T11:05:08.533498Z","iopub.status.idle":"2023-01-05T11:05:29.048489Z","shell.execute_reply.started":"2023-01-05T11:05:08.533445Z","shell.execute_reply":"2023-01-05T11:05:29.047364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the overall loss and accuracy\nmodel.evaluate(X_test, y_test, verbose=1) ","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:05:29.050153Z","iopub.execute_input":"2023-01-05T11:05:29.050607Z","iopub.status.idle":"2023-01-05T11:05:49.564339Z","shell.execute_reply.started":"2023-01-05T11:05:29.050556Z","shell.execute_reply":"2023-01-05T11:05:49.563387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict(X_test)\ny_pred = predicted.argmax(axis=-1)\n\nprint(classification_report(le.transform(df_test['Emotion']), y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:05:49.566592Z","iopub.execute_input":"2023-01-05T11:05:49.566966Z","iopub.status.idle":"2023-01-05T11:06:09.436293Z","shell.execute_reply.started":"2023-01-05T11:05:49.566929Z","shell.execute_reply":"2023-01-05T11:06:09.435232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize Loss & Accuracy\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:06:14.865323Z","iopub.execute_input":"2023-01-05T11:06:14.86563Z","iopub.status.idle":"2023-01-05T11:06:15.437286Z","shell.execute_reply.started":"2023-01-05T11:06:14.865602Z","shell.execute_reply":"2023-01-05T11:06:15.436413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, you may asking when I give a sentence to the model how the classification will done?**\n\n1. First, the model will convert the words in the sentence into numerical representations, also known as word embeddings. These word embeddings capture the meaning of the words and their relationships to each other in a continuous vector space.\n\n2. The word embeddings for each word in the sentence are then input to two separate LSTM models: one that processes the sequence forwards and one that processes it backwards. At each time step, each LSTM processes the current word embedding and the previous hidden state to produce a new hidden state. The hidden state at each time step is a combination of the previous hidden state and the current input, and it captures the information from all the previous time steps in the sequence.\n\n3. After processing all the words in the sentence, the two LSTMs will have final hidden states that encode the meaning of the sentence from both the forwards and backwards directions. These final hidden states are then combined and used to make a prediction.","metadata":{}},{"cell_type":"code","source":"# Classify custom sample\n\nsentences = [\n            \"He's over the moon about being accepted to the university\",\n            \"Your point on this certain matter made me outrageous, how can you say so? This is insane.\",\n            \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n            \"Merlin's beard harry, you can cast the Patronus charm! I'm amazed!\"\n            ]\nfor sentence in sentences:\n    print(sentence)\n    sentence = normalized_sentence(sentence)\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=229, truncating='pre')\n    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n    proba =  np.max(model.predict(sentence))\n    print(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:06:09.43792Z","iopub.execute_input":"2023-01-05T11:06:09.438776Z","iopub.status.idle":"2023-01-05T11:06:12.543735Z","shell.execute_reply.started":"2023-01-05T11:06:09.438728Z","shell.execute_reply":"2023-01-05T11:06:12.542518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence= 'my old brother is dead'\nprint(sentence)\nsentence = normalized_sentence(sentence)\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen=229, truncating='pre')\nresult = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\nproba =  np.max(model.predict(sentence))\nprint(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:11:12.863279Z","iopub.execute_input":"2023-01-05T11:11:12.864269Z","iopub.status.idle":"2023-01-05T11:11:13.581559Z","shell.execute_reply.started":"2023-01-05T11:11:12.864217Z","shell.execute_reply":"2023-01-05T11:11:13.58047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence= 'Im feeling sad today'\nprint(sentence)\nsentence = normalized_sentence(sentence)\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen=229, truncating='pre')\nresult = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\nproba =  np.max(model.predict(sentence))\nprint(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:11:27.465652Z","iopub.execute_input":"2023-01-05T11:11:27.466077Z","iopub.status.idle":"2023-01-05T11:11:28.171402Z","shell.execute_reply.started":"2023-01-05T11:11:27.466043Z","shell.execute_reply":"2023-01-05T11:11:28.170175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('Emotion Recognition From English text.h5')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T11:06:14.756595Z","iopub.execute_input":"2023-01-05T11:06:14.756967Z","iopub.status.idle":"2023-01-05T11:06:14.859618Z","shell.execute_reply.started":"2023-01-05T11:06:14.756929Z","shell.execute_reply":"2023-01-05T11:06:14.858529Z"},"trusted":true},"execution_count":null,"outputs":[]}]}